#### Averaging Weights Leads to Wider Optima and Better Generalization

*Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson*

**Summary:** Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training.

**ArXiv:** [1803.05407](https://arxiv.org/abs/1803.05407), [Local link](Papers/1803.05407v3_Stochastic_Weight_Averaging.pdf), Hash: ee58cbd9aa002307de991da7560cb979, *Added: 2024-09-12* 

#### World Models

*David Ha, JÃ¼rgen Schmidhuber*

**Summary:** We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment.

**ArXiv:** [1803.10122](https://arxiv.org/abs/1803.10122), [Local link](Papers/1803.10122.pdf), Hash: 9df6f092e3faf2105c0d87e5477d5c67, *Added: 2024-09-12* 

#### TRAINED TERNARY QUANTIZATION

*Chenzhuo Zhu, Song Han, Huizi Mao, William J. Dally*

**Summary:** Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values.

**ArXiv:** [1612.01064](https://arxiv.org/abs/1612.01064), [Local link](Papers/1612.01064v3.pdf), Hash: 3c9b0ed9eca03d76395f2c6afdf3e219, *Added: 2024-09-12* 

#### Proximal Policy Optimization Algorithms

*John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov*

**Summary:** We propose a new family of policy gradient methods for reinforcement learning, which al- ternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent.

**ArXiv:** [1707.06347](https://arxiv.org/abs/1707.06347), [Local link](Papers/1707.06347v2_PPO.pdf), Hash: 9e0725fc34e41616c0faab684513e464, *Added: 2024-09-12* 

#### SqueezeNext: Hardware-Aware Neural Network Design

*Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, Kurt Keutzer*

**Summary:** One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural networks.

**ArXiv:** [1803.10615](https://arxiv.org/abs/1803.10615), [Local link](Papers/1803.10615v2_squeezenext.pdf), Hash: ad6c65774f83c471c4529189e69999c6, *Added: 2024-09-12* 

#### Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions

*Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, Yingyan Lin*

**Summary:** We proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weight-sharing, by only recording Kcluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed k-means regularization, which tends to make hard assignments of convolutional layer weights to Klearned cluster centers during re-training.

**ArXiv:** [1806.09228](https://arxiv.org/abs/1806.09228), [Local link](Papers/1806.09228v1_palettization.pdf), Hash: 3988217769b90c5ec8a068684e558d8c, *Added: 2024-09-12* 


#### Learned Step Size Quantization

*Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S. Modha*

**Summary:** Learned Step Size Quantization presents a method for training deep networks with low precision operations at inference time, achieving high accuracy on the ImageNet dataset when using models with weights and activations quantized to 2-, 3- or 4-bits of precision.

**ArXiv:** [1902.08153](https://arxiv.org/abs/1902.08153), [Local link](Papers/1902.08153v3_good_paper_qat.pdf), Hash: 601fe34a0b921f764bd1c091b25b024b, *Added: 2024-09-12* 

#### All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification

*Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu*

**Summary:** Shift operation is an efficient alternative over depthwise separable convolution. However, it is still bottlenecked by its implementation manner, namely memory movement. To put this direction forward, a new and novel basic component named Sparse Shift Layer (SSL) is introduced in this paper to construct efficient convolutional neural networks.

**ArXiv:** [1903.05285](https://arxiv.org/abs/1903.05285), [Local link](Papers/1903.05285.pdf), Hash: b9e14495a718204775dd13877ae5c5a5, *Added: 2024-09-12* 

#### Character Region Awareness for Text Detection

*Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee*

**Summary:** This paper proposes a new scene text detection method that effectively detects text areas by exploring each character and affinity between characters.

**ArXiv:** [1904.01941](https://arxiv.org/abs/1904.01941), [Local link](Papers/1904.01941v1_craft_text_detection.pdf), Hash: c78b0a8bcb1f041fe659c25cd16832f9, *Added: 2024-09-12* 

#### Searching for MobileNetV3

*Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam*

**Summary:** We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design.

**ArXiv:** [1905.02244](https://arxiv.org/abs/1905.02244), [Local link](Papers/1905.02244v5_mobilenetv3.pdf), Hash: 6f29ad563a89d1086adf5e7f990c6e0a, *Added: 2024-09-12* 

#### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

*Mingxing Tan, Quoc V. Le*

**Summary:** Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.

**ArXiv:** [1905.11946](https://arxiv.org/abs/1905.11946), [Local link](Papers/1905.11946_efficientnet.pdf), Hash: a6491e929ceb9af0db1f063a6a69fb61, *Added: 2024-09-12* 

#### DeepShift: Towards Multiplication-Less Neural Networks

*Mostafa Elhoushi, Zihao Chen, Farhan Shafig, Ye Henry Tian, Joey Yiwei Li*

**Summary:** The high computation, memory, and power budgets of inferring convolutional neural networks (CNNs) are major bottlenecks of model deployment to edge computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is time and energy-intensive even on high-grade servers.

**ArXiv:** [1905.13298](https://arxiv.org/abs/1905.13298), [Local link](Papers/1905.13298_deepshift.pdf), Hash: 94eeb0e12a131bbd5e31cec15e40b281, *Added: 2024-09-12* 

#### ADDITIVE POWERS -OF-TWO QUANTIZATION : AN EFFICIENT NON-UNIFORM DISCRETIZATION FOR NEURAL NETWORKS

*Yuhang Li, Xin Dong, Wei Wang*

**Summary:** We propose Additive Powers-of-Two (APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks.

**ArXiv:** [1909.13144](https://arxiv.org/abs/1909.13144), [Local link](Papers/1909.13144v2_expcoding.pdf), Hash: 614a358832156d33df4e2f86e8360e2c, *Added: 2024-09-12* 

#### AdderNet: Do We Really Need Multiplications in Deep Learning?

*Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu*

**Summary:** Adder Networks (AdderNets) trade massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs.

**ArXiv:** [1912.13200](https://arxiv.org/abs/1912.13200), [Local link](Papers/1912.13200.pdf), Hash: 8567dcb1010414feaac0223952396917, *Added: 2024-09-12* 

#### AdderSR: Towards Energy Efficient Image Super-Resolution

*Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, Dacheng Tao*

**Summary:** This paper studies the single image super-resolution problem using adder neural networks (AdderNets) and proposes AdderSR to achieve comparable performance with CNN baselines while reducing energy consumption by about 2.5 times.

**ArXiv:** [2009.08891](https://arxiv.org/abs/2009.08891), [Local link](Papers/2009.08891.pdf), Hash: ed2b2c74df22b3359386cf86ce266683, *Added: 2024-09-12* 

#### ANIMAGE IS WORTH 16X16 W ORDS: Transformers for Image Recognition at Scale

*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby*

**Summary:** We show that the reliance on CNNs in vision is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.

**ArXiv:** [2010.11929](https://arxiv.org/abs/2010.11929), [Local link](Papers/2010.11929.pdf), Hash: 7f1ba6e93e08ec7fc3f4fa1c8a255ac0, *Added: 2024-09-12* 

#### Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks

*TORSTEN HOEFLER, DAN ALISTARH, TAL BEN-NUN, NIKOLI DRYDEN, ALEXANDRA PESTE*

**Summary:** The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks.

**ArXiv:** [2102.00554](https://arxiv.org/abs/2102.00554), [Local link](Papers/2102.00554v1_sparsity.pdf), Hash: 2220739fe6d99860c784327d505b186d, *Added: 2024-09-12* 

#### VS-Q UANT: PER-VECTOR SCALED QUANTIZATION FOR ACCURATE LOW-PRECISION NEURAL NETWORK INFERENCE

*Steve Dai, Rangharajan Venkatesan, Haoxing Ren, Brian Zimmer, William J. Dally, Brucek Khailany*

**Summary:** Quantization enables efficient acceleration of deep neural networks by reducing model memory footprint and exploiting low-cost integer math hardware units.

**ArXiv:** [2102.04503](https://arxiv.org/abs/2102.04503), [Local link](Papers/2102.04503v1_per_vector_quant.pdf), Hash: b7fdb71333c901b39ecfc058d564e105, *Added: 2024-09-12* 

#### Network Quantization with Element-wise Gradient Scaling

*Junghyup Lee, Dohyung Kim, Bumsub Ham*

**Summary:** Network quantization aims at reducing bit-widths of weights and/or activations, particularly important for implementing deep neural networks with limited hardware resources. Most methods use the straight-through estimator (STE) to train quantized networks, which avoids a zero-gradient problem by replacing a derivative of a discretizer (i.e., a round function) with that of an identity function.

**ArXiv:** [2104.00903](https://arxiv.org/abs/2104.00903), [Local link](Papers/2104.00903v1_STE_Optimization_EWGS.pdf), Hash: b54c7ede8c2bafc99ef592b057b66e9d, *Added: 2024-09-12* 

#### LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference

*Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, HervÃ© JÃ©gou, Matthijs Douze*

**Summary:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware.

**ArXiv:** [2104.01136](https://arxiv.org/abs/2104.01136), [Local link](Papers/2104.01136v2_levit_light_weight_vision.pdf), Hash: c74f4b03484329d133a1188ab9c8bd4b, *Added: 2024-09-12* 

#### DKM: Differentiable k-Means Clustering Layer for Neural Network Compression

*Minsik Cho, Keivan Alizadeh-Vahid, Saurabh Adya, Mohammad Rastegari*

**Summary:** We propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight-clustering for DNN model compression.

**ArXiv:** [2108.12659v4](https://arxiv.org/abs/2108.12659v4), [Local link](Papers/2108.12659v4_palettization.pdf), Hash: 806791b8719a1735064e32741d0db5b7, *Added: 2024-09-12* 

#### Primer: Searching for Efficient Transformers for Language Modeling

*David R. So, Wojciech MaÅke, Hanxiao Liu, Zihang Dai, Noam Shazeer, Quoc V. Le*

**Summary:** Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant.

**ArXiv:** [2109.08668](https://arxiv.org/abs/2109.08668), [Local link](Papers/2109.08668v2_primer_llm_architecture_search.pdf), Hash: b0b2d5391fbd2c9ce4bdf4a29f18ef67, *Added: 2024-09-12* 

#### Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training

*Charbel Sakr, Steve Dai, Rangharajan Venkatesan, Brian Zimmer, William J. Dally, Brucek Khailany*

**Summary:** We propose Optimally Clipped Tensors And Vectors (OCTA V), a recursive algorithm to determine MSE-optimal clipping scalars for improved quantization-aware training, and reveal limitations in common gradient estimation techniques proposing magnitude-aware differentiation as a remedy.

**ArXiv:** [2206.06501](https://arxiv.org/abs/2206.06501), [Local link](Papers/2206.06501v1_optimal_clipping.pdf), Hash: 55168ecaa41e3f8fb55f87d69c766b7e, *Added: 2024-09-12* 

#### LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale

*Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer*

**Summary:** We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance.

**ArXiv:** [2208.07339](https://arxiv.org/abs/2208.07339), [Local link](Papers/2208.07339.pdf), Hash: cde5fd566b995573932664907c0eea57, *Added: 2024-09-12* 

#### A Curate Post-Training Quantization for Generative Pre-Trained Transformers

*Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh*

**Summary:** GPTQ, a new one-shot weight quantization method based on approximate second-order information, is proposed to reduce the bitwidth of GPT models with minimal accuracy degradation. This allows execution of an 175 billion-parameter model inside a single GPU for generative inference.

**ArXiv:** [2210.17323](https://arxiv.org/abs/2210.17323), [Local link](Papers/2210.17323.pdf), Hash: 1188fd9e7933a11cf51b835ac07aaa13, *Added: 2024-09-12* 

#### A Curate Post-Training Quantization for Generative Pre-Trained Transformers

*Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh*

**Summary:** GPTQ, a new one-shot weight quantization method based on approximate second-order information, efficiently reduces the bitwidth of GPT models to 3 or 4 bits per weight with negligible accuracy degradation.

**ArXiv:** [2210.17323](https://arxiv.org/abs/2210.17323), [Local link](Papers/2210.17323v2_GPTQ.pdf), Hash: 1188fd9e7933a11cf51b835ac07aaa13, *Added: 2024-09-12* 

#### Symbolic Discovery of Optimization Algorithms

*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*

**Summary:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training.

**ArXiv:** [2302.06675](https://arxiv.org/abs/2302.06675), [Local link](Papers/2302.06675v4_Lion_Opitmizer.pdf), Hash: 03ef2f8cf9b9a6b67b177bf857f5f057, *Added: 2024-09-12* 

#### Multiplication-Free Transformer Training via Piecewise Affine Operations

*Atli Kosson, Martin Jaggi*

**Summary:** We replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters.

**ArXiv:** [2305.17190](https://arxiv.org/abs/2305.17190), [Local link](Papers/2305.17190v2_mulfree.pdf), Hash: 269d214383aa4d76bb0d948da821b526, *Added: 2024-09-12* 

#### A WQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

*Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, Song Han*

**Summary:** This paper proposes Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for Large Language Model weight-only quantization, that searches for optimal per-channel scaling based on the observation of activation to protect salient weights.

**ArXiv:** [2306.00978](https://arxiv.org/abs/2306.00978), [Local link](Papers/2306.00978.pdf), Hash: e37ecfa8af69a03b4e126934caf4cee3, *Added: 2024-09-12* 

#### Patch nâ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution

*Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario LuÄiÄ, Neil Houlsby*

**Summary:** NaViT(Native Resolution ViT) uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios, demonstrating improved training efficiency for large-scale supervised and contrastive image-text pretraining.

**ArXiv:** [2302.08313](https://arxiv.org/abs/2302.08313), [Local link](Papers/2307.06304.pdf), Hash: afbbc25f4c7825c575cc757c566cd4f5, *Added: 2024-09-12* 

#### Explaining Grokking Through Circuit Efficiency

*Vikrant Varma, Rohin Shah, Zachary Kenton, JÃ¡nos KramÃ¡r, Ramana Kumar*

**Summary:** We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.

**ArXiv:** [2309.02390](https://arxiv.org/abs/2309.02390), [Local link](Papers/2309.02390v1_grokking.pdf), Hash: 11aa974cb9279a9217167206b42451f4, *Added: 2024-09-12* 

#### BitNet: Scaling 1-bit Transformers for Large Language Models

*Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei*

**Summary:** We introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. It reduces memory footprint and energy consumption while maintaining competitive performance.

**ArXiv:** [2310.11453](https://arxiv.org/abs/2310.11453), [Local link](Papers/2310.11453_bitnet.pdf), Hash: 2ad51decc58a80910bbd05b8084c676a, *Added: 2024-09-12* 

#### FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design

*Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song*

**Summary:** Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications.

**ArXiv:** [2401.14112](https://arxiv.org/abs/2401.14112), [Local link](Papers/2401.14112v2_fp6_llm.pdf), Hash: 7dbfccd4df499b1e6b6fd59857fe094e, *Added: 2024-09-12* 

#### MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

*Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra*

**Summary:** This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment.

**ArXiv:** [2402.14905](https://arxiv.org/abs/2402.14905), [Local link](Papers/2402.14905v2_mobilellm.pdf), Hash: 0fe2ef51d464a5cae1876f2382379428, *Added: 2024-09-12* 

#### The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

*Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei*

**Summary:** Recent research, such as BitNet [ WMD+23], is paving the way for a new era of 1- bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58 , in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.

**ArXiv:** [2402.17764](https://arxiv.org/abs/2402.17764), [Local link](Papers/2402.17764_1_58bit.pdf), Hash: 45f0b9c45de94bd864bd260cc10f40eb, *Added: 2024-09-12* 

#### The Unreasonable Ineffectiveness of the Deeper Layers

*Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts*

**Summary:** We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed.

**ArXiv:** [2403.17887](https://arxiv.org/abs/2403.17887), [Local link](Papers/2403.17887v1_layer_pruning.pdf), Hash: 652bf63701a5726bcffd722e555344b4, *Added: 2024-09-12* 

#### Tutorial on Diffusion Models for Imaging and Vision

*Stanley Chan*

**Summary:** The goal of this tutorial is to discuss the essential ideas underlying these diffusion models.

**ArXiv:** [2403.18103](https://arxiv.org/abs/2403.18103), [Local link](Papers/2403.18103v2_Tutorial_on_Diffusion_Models.pdf), Hash: bf04ad5ff022048df73bf8c3608ee803, *Added: 2024-09-12* 

#### Mixture-of-Depths: Dynamically allocating Compute in Transformer-Based Language Models

*David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro*

**Summary:** In this work, we demonstrate that transformers can learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimizing the allocation along the sequence for different layers across the model depth.

**ArXiv:** [2404.02258](https://arxiv.org/abs/2404.02258), [Local link](Papers/2404.02258.pdf), Hash: f90acf2f1085d7d99cd7c9f6aa908008, *Added: 2024-09-12* 

#### Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction

*Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang*

**Summary:** We present Visual AutoRegressive modeling (V AR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: V AR, for the first time , makes GPT-style AR models surpass diffusion transformers in image generation.

**ArXiv:** [2404.02905](https://arxiv.org/abs/2404.02905), [Local link](Papers/2404.02905.pdf), Hash: 4016f04c122748c124df09fe18b68492, *Added: 2024-09-12* 

#### Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference

*Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen GÃ¶rtler, Dominik Moritz, Jeffrey P. Bigham, Zhile Ren, Cecile Foret, Qi Shan, Xiaoyi Zhang*

**Summary:** On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences.

**ArXiv:** [2404.03085](https://arxiv.org/abs/2404.03085), [Local link](Papers/2404.03085v1_talaria.pdf), Hash: 8f4f4f0eda22a495f23c1337142e775b, *Added: 2024-09-12* 

#### Training LLMs over Neurally Compressed Text

*Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant*

**Summary:** In this paper, we explore the idea of training large language models (LLMs) over highly compressed text using Equal-Info Windows, a novel compression technique that segments text into blocks compressing to the same bit length.

**ArXiv:** [2404.03626](https://arxiv.org/abs/2404.03626), [Local link](Papers/2404.03626.pdf), Hash: 53529e681ab369bfee9e63d32619212c, *Added: 2024-09-12* 

#### Physics of Language Models: Part 3.3 Knowledge Capacity Scaling Laws

*Zeyuan Allen-Zhu, Yuanzhi Li*

**Summary:** Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores.

**ArXiv:** [2404.05405](https://arxiv.org/abs/2404.05405), [Local link](Papers/2404.05405v1_physics_of_llm.pdf), Hash: 682a50a5b5813643d0afc1a494abe7fc, *Added: 2024-09-12* 

#### Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization

*Boshi Wang, Xiang Yue, Yu Su, Huan Sun*

**Summary:** We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers canlearn implicit reasoning, but only through grokking , i.e., extended training far beyond overfitting.

**ArXiv:** [2405.15071](https://arxiv.org/abs/2405.15071), [Local link](Papers/2405.15071v1_grokking.pdf), Hash: 259f8dbea45f5563ea5f2d9b166d44ec, *Added: 2024-09-12* 

#### Scalable MatMul-free Language Modeling

*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian*

**Summary:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales.

**ArXiv:** [2406.02528](https://arxiv.org/abs/2406.02528), [Local link](Papers/2406.02528v1_matmul_free.pdf), Hash: dc43e3bf03f0f3882f1e15b331027f19, *Added: 2024-09-12* 

#### Q-Sparse: All Large Language Models can be Fully Sparsely-Activated

*Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei*

**Summary:** We introduce, Q-Sparse , a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of acti-vations in LLMs which can bring significant efficiency gains in inference.

**ArXiv:** [2407.10969](https://arxiv.org/abs/2407.10969), [Local link](Papers/2407.10969v1_Q-sparse.pdf), Hash: fd79b84fc95a6af58f65017ab49255fa, *Added: 2024-09-12* 

#### ShiftAddNet: A Hardware-Inspired Deep Network

*Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, Yingyan Lin*

**Summary:** This paper introduces ShiftAddNet, a deep network inspired by energy-efficient hardware implementation that performs multiplication using additions and logical bit-shifts, resulting in energy-efficient inference and training without compromising expressive capacity.

**ArXiv:** [1905.13298](https://arxiv.org/abs/1905.13298), [Local link](Papers/NeurIPS-2020-shiftaddnet-a-hardware-inspired-deep-network-Paper.pdf), Hash: bae3561ed2129c272887af8ce50df626, *Added: 2024-09-12* 

#### Optimal Brain Damage

*Yann Le Cun, John S. Denker, Sara A. Solla*

**Summary:** We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network.

**ArXiv:** N/A, [Local link](Papers/NIPS-1989-optimal-brain-damage-Paper.pdf), Hash: c1bfc00c11a88f7d9abbc1615f100613, *Added: 2024-09-12* 

#### OCP Microscaling Formats (MX) Specification

*Bita Darvish Rouhani, Nitin Garegrat, Tom Savell, Ankit More, Kyung-Nam Han, Ritchie Zhao, Mathew Hall, Jasmine Klar, Eric Chung, Yuan Yu, Microsoft, Michael Schulte, Ralph Wittig, Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Pradeep Dubey, Marius Cornea, Alexander Heinecke, Andres Rodriguez, Martin Langhammer, Summer Deng, Maxim Naumov, Paulius Micikevicius, Michael Siu, Colin Verrilli*

**Summary:** This document describes the OCP Microscaling Formats (MX) Specification, which includes definitions, word usage, overview, basic operations, and more.

**ArXiv:** N/A, [Local link](Papers/OCP_Microscaling%20Formats%20(MX)%20v1.0%20Spec_Final.pdf), Hash: 9a52e4a8d4a28b49ecef2a3a3f918337, *Added: 2024-09-12* 

#### Improving Neural Network Quantization without Retraining using Outlier Channel Splitting

*Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang*

**Summary:** Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training.

**ArXiv:** [1901.09504](https://arxiv.org/abs/1901.09504), [Local link](Papers/1901.09504v3_outlier%20channel%20splitting.pdf), Hash: 745960627406b452459035a5285e7824, *Added: 2024-09-12* 

#### DKM: Differentiable k-Means Clustering Layer for Neural Network Compression

*Minsik Cho, Keivan Alizadeh-Vahid, Saurabh Adya, Mohammad Rastegari*

**Summary:** This paper proposes a differentiable k-means clustering layer (DKM) and its application to train-time weight-clustering for deep neural network model compression. DKM enables joint optimization of the DNN parameters and clustering centroids, providing superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks.

**ArXiv:** [2108.12659v4](https://arxiv.org/abs/2108.12659v4), [Local link](Papers/2108.12659v4_palettization.pdf), Hash: 806791b8719a1735064e32741d0db5b7, *Added: 2024-09-12* 

#### Rare Gems: Finding Lottery Tickets at Initialization

*Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grindenwalt Nagle, Hongyi Wang, Eric Xing, Kangwook Lee, Dimitris Papailiopoulos*

**Summary:** GEM-MINER finds lottery tickets at initialization that beat current baselines, finding trainable networks to accuracy competitive or better than Iterative Magnitude Pruning (IMP), and does so up to 19 times faster.

**ArXiv:** [2202.12002](https://arxiv.org/abs/2202.12002), [Local link](Papers/2202.12002v2_Finding%20Lottery%20Tickets%20at%20Initialization.pdf), Hash: 4823419ab710161dca0fd8515e29582f, *Added: 2024-09-12* 

#### No Free Prune: Information-Theoretic Barriers to Pruning at Initialization

*Tanishq Kumar, Kevin Luo, Mark Sellke*

**Summary:** The existence of lottery tickets Frankle & Carbin (2018) at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ('pruning at initialization') have been broadly unsuccessful Frankle et al. (2020b).

**ArXiv:** [2402.01089](https://arxiv.org/abs/2402.01089), [Local link](Papers/2402.01089v1_No%20Free%20Prune.pdf), Hash: e6582e9efd12f859cbd8be9953195142, *Added: 2024-09-12* 

#### Least Squares Quantization in PCM

*Stuart P. Lloyd*

**Summary:** In this paper, the author derives necessary conditions for optimum finite quantization schemes with a given ensemble of signals to handle, using the optimization criterion that the average quantization noise power be a minimum.

**ArXiv:** N/A, [Local link](Papers/lloyd1982_least_squares_quant.pdf), Hash: e8cfe41ea5354727e98fe6ffd6d7e2e1, *Added: 2024-09-12* 

#### Optimal Brain Damage

*Yann Le Cun, John S. Denker, Sara A. Solla*

**Summary:** We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network.

**ArXiv:** N/A, [Local link](Papers/NIPS-1989-optimal-brain-damage-Paper.pdf), Hash: c1bfc00c11a88f7d9abbc1615f100613, *Added: 2024-09-12* 

#### OCP Microscaling Formats (MX) Specification

*Bita Darvish Rouhani, Nitin Garegrat, Tom Savell, Ankit More, Kyung-Nam Han, Ritchie Zhao, Mathew Hall, Jasmine Klar, Eric Chung, Yuan Yu, Microsoft, Michael Schulte, Ralph Wittig, Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Pradeep Dubey, Marius Cornea, Alexander Heinecke, Andres Rodriguez, Martin Langhammer, Summer Deng, Maxim Naumov, Paulius Micikevicius, Michael Siu, Colin Verrilli*

**Summary:** This Specification defines the Microscaling (MX) format for use in Open Compute Project (OCP) designs to improve efficiency and performance.

**ArXiv:** N/A, [Local link](Papers/OCP_Microscaling%20Formats%20%28MX%29%20v1.0%20Spec_Final.pdf), Hash: 9a52e4a8d4a28b49ecef2a3a3f918337, *Added: 2024-09-12* 

#### REFT: Reasoning with Reinforced Fine-Tuning

*Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li*

**Summary:** We propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning Large Language Models for reasoning, with math problem-solving as an example.

**ArXiv:** [2401.08967](https://arxiv.org/abs/2401.08967), [Local link](Papers/2401.08967v2_REFT.pdf), Hash: 145e3eae6b874df8ba074c18badabd71, *Added: 2024-09-13* 

#### Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

*Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang*

**Summary:** In this paper, we propose R3: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models.

**ArXiv:** [2402.05808](https://arxiv.org/abs/2402.05808), [Local link](Papers/2402.05808v2_Reverse%20Curriculum%20Reinforcement%20Learning.pdf), Hash: 42efd41e64ac26f1c0c57f9eb44fccd1, *Added: 2024-09-13* 

#### Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

*Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman*

**Summary:** We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions.

**ArXiv:** [2403.09629](https://arxiv.org/abs/2403.09629), [Local link](Papers/2403.09629v2_Quiet-STaR.pdf), Hash: 8cdec7acbfc428496678eee2a098fd1a, *Added: 2024-09-13* 

#### Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection

*Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee*

**Summary:** To overcome challenges in aligning and improving large language models (LLMs) with human preferences, Reinforcement Learning from Reflective Feedback (RLRF) is proposed. It leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs.

**ArXiv:** [2403.14238](https://arxiv.org/abs/2403.14238), [Local link](Papers/2403.14238v1_Reinforcement%20Learning%20from%20Reflective%20Feedback.pdf), Hash: 638fd83a2afcd870acde9aebfc69fe34, *Added: 2024-09-13* 

#### Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

*Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou*

**Summary:** We explore how generating a chain of thought âa series of intermediate reasoning stepsâsignificantly improves the ability of large language models to perform complex reasoning.

**ArXiv:** [2306.04111](https://arxiv.org/abs/2306.04111), [Local link](Papers/2201.11903v6_Chain-of-Thought.pdf), Hash: bb5cb052621074b4426998fcfeee100b, *Added: 2024-09-14* 

#### STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning

*Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman*

**Summary:** We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning.

**ArXiv:** [2203.14465](https://arxiv.org/abs/2203.14465), [Local link](Papers/2203.14465v2_STaR_Bootstrapping%20Reasoning%20With%20Reasoning.pdf), Hash: 954323e019ed7433cea0c9c4d466b810, *Added: 2024-09-14* 

#### Let's Verify Step by Step

*Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe*

**Summary:** In this paper, we compare process supervision and outcome supervision for training reliable language models to solve multi-step reasoning problems. Our investigation shows that process supervision significantly outperforms outcome supervision when applied to the challenging MATH dataset.

**ArXiv:** [2305.20050](https://arxiv.org/abs/2305.20050), [Local link](Papers/2305.20050v1_Let%E2%80%99s%20Verify%20Step%20by%20Step.pdf), Hash: 7b7306d1edb567ccf282872ca7cc58f6, *Added: 2024-09-14* 

#### Training Chain-of-Thought via Latent-Variable Inference

*Du Phan, Matthew D. Hoffman, David Dohan, Sholto Douglas Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A. Saurous*

**Summary:** We propose a fine-tuning strategy that tries to maximize the marginal log-likelihood of generating a correct answer using CoT prompting, approximately averaging over all possible rationales.

**ArXiv:** [2304.08306](https://arxiv.org/abs/2304.08306), [Local link](Papers/2312.02179v1.pdf), Hash: d1bcfde44450e930723172a74b4bc72b, *Added: 2024-09-15* 

#### Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

*Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang*

**Summary:** In this paper, we propose R3: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models.

**ArXiv:** [2402.05808](https://arxiv.org/abs/2402.05808), [Local link](Papers/2402.05808v2.pdf), Hash: 42efd41e64ac26f1c0c57f9eb44fccd1, *Added: 2024-09-15* 

#### ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback

*Ju-Seung Byun, Jiyun Chun, Jihyung Kil, Andrew Perrault*

**Summary:** We propose a two-stage algorithm ARES that Alternates REinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to enhance multi-modal chain-of-thought reasoning through diverse AI feedback.

**ArXiv:** [2407.00087](https://arxiv.org/abs/2407.00087), [Local link](Papers/2407.00087v1.pdf), Hash: c8145601b30ea1f059cb8894448c3b9e, *Added: 2024-09-15* 

#### Large Language Monkeys: Scaling Inference Compute with Repeated Sampling

*Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher RÃ©, Azalia Mirhoseini*

**Summary:** Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples.

**ArXiv:** [2407.21787](https://arxiv.org/abs/2407.21787), [Local link](Papers/2407.21787v1_Scaling%20Inference%20Compute%20with%20repeated%20sampling.pdf), Hash: 54d27c34abf8b6b2893bc0646bc32740, *Added: 2024-09-15* 

#### Deep Unsupervised Learning using Nonequilibrium Thermodynamics

*Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli*

**Summary:** A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable.

**ArXiv:** [1503.03585](https://arxiv.org/abs/1503.03585), [Local link](Papers/1503.03585v8_Deep%20Unsupervised%20Learning%20using%20Nonequilibrium%20Thermodynamics.pdf), Hash: b1390adda57bbb669fa958add52328c3, *Added: 2024-09-29* 

#### Chain of Thought Empowers Transformers to Solve Inherently Serial Problems

*Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma*

**Summary:** Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks.

**ArXiv:** [2402.12875](https://arxiv.org/abs/2402.12875), [Local link](Papers/2402.12875v3_Chain%20of%20Thought%20Empowers%20Transformers.pdf), Hash: 2bfaabd7d8dea75c933e4b3eab2f66b0, *Added: 2024-09-29* 

#### ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization

*Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan (Celine) Lin*

**Summary:** We propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors.

**ArXiv:** [2406.05981](https://arxiv.org/abs/2406.05981), [Local link](Papers/2406.05981v3_ShiftAddLLM.pdf), Hash: a0824c77d773f1e6438ac1bed7fde146, *Added: 2024-09-29* 

#### Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Model Parameters

*Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar*

**Summary:** In this paper, we study the scaling of inference-time computation in LLMs and find that a 'compute-optimal' scaling strategy can improve test-time compute efficiency by more than 4x compared to a best-of-N baseline.

**ArXiv:** [2408.03314](https://arxiv.org/abs/2408.03314), [Local link](Papers/2408.03314v1_Scaling%20LLM%20Test-Time%20Compute%20Optimally.pdf), Hash: 90d86e47d5568dbce5599bc323c1140b, *Added: 2024-09-29* 

#### Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning

*Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, Oktay Goktas*

**Summary:** Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. The Iteration of Thought (IoT) framework enhances LLM responses by generating 'thought'-provoking prompts vis a vis an input query and the current iteration of an LLM's response.

**ArXiv:** [2409.12618](https://arxiv.org/abs/2409.12618), [Local link](Papers/2409.12618v1_Iteration%20of%20Thought.pdf), Hash: 63851d16d472a122bc6875070a6615df, *Added: 2024-09-29* 

#### Training Language Models to Self-Correct via Reinforcement Learning

*Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, JD Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust*

**Summary:** Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision.

**ArXiv:** [2409.12917](https://arxiv.org/abs/2409.12917), [Local link](Papers/2409.12917v1_Training%20Language%20Models%20to%20Self-Correct%20via.pdf), Hash: 5d2d16a325b89d46187267d35e2ca0b0, *Added: 2024-09-29* 

#### Chain-of-Thought Reasoning without Prompting

*Xuezhi Wang, Denny Zhou*

**Summary:** Our study explores whether large language models can reason effectively without prompting by altering the decoding process to uncover chain-of-thought reasoning paths that are frequently inherent in top-k alternative token sequences.

**ArXiv:** [2402.10200](https://arxiv.org/abs/2402.10200), [Local link](Papers/2402.10200v2_Chain-of-Thought%20Reasoning%20without%20Prompting.pdf), Hash: 5fe4440b5ccee481c078eda2a2cf0575, *Added: 2024-10-18* 

#### GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models

*Iman Mirzadeh, Keivan Alizadeh Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar*

**Summary:** This paper introduces GSM-Symbolic, an improved benchmark for evaluating the mathematical reasoning capabilities of large language models. The study reveals that LLMs exhibit noticeable variance when responding to different instantiations of questions and their performance declines significantly as the number of clauses in a question increases.

**ArXiv:** [2410.05229](https://arxiv.org/abs/2410.05229), [Local link](Papers/2410.05229v1_GSM-Symbolic.pdf), Hash: 0116c4a8ea5e9c013334193b2aed1ef9, *Added: 2024-10-18* 

#### ÂµNAS: Constrained Neural Architecture Search for Microcontrollers

*Edgar Liberis, Åukasz Dudziak, Nicholas D. Lane*

**Summary:** IoT devices are powered by microcontroller units (MCUs) which are extremely resource-scarce: a typical MCU may have an underpowered processor and around 64 KB of mem-ory and persistent storage. Designing neural networks for such a platform requires an intricate balance between keep-ing high predictive performance (accuracy) while achieving low memory and storage usage and inference latency.

**ArXiv:** [2010.11267](https://arxiv.org/abs/2010.11267), [Local link](Papers/3437984.3458836_%CE%BCNAS.pdf), Hash: 97de48ec413ce403e8bfb384cf2d79e1, *Added: 2024-10-18* 

