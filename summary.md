#### Averaging Weights Leads to Wider Optima and Better Generalization

*Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson*

**Summary:** Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training.

**ArXiv:** [1803.05407](https://arxiv.org/abs/1803.05407), [Local link](Papers/1803.05407v3_Stochastic_Weight_Averaging.pdf), Hash: ee58cbd9aa002307de991da7560cb979, *Added: 2024-09-12* 

#### World Models

*David Ha, JÃ¼rgen Schmidhuber*

**Summary:** We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment.

**ArXiv:** [1803.10122](https://arxiv.org/abs/1803.10122), [Local link](Papers/1803.10122.pdf), Hash: 9df6f092e3faf2105c0d87e5477d5c67, *Added: 2024-09-12* 

#### TRAINED TERNARY QUANTIZATION

*Chenzhuo Zhu, Song Han, Huizi Mao, William J. Dally*

**Summary:** Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values.

**ArXiv:** [1612.01064](https://arxiv.org/abs/1612.01064), [Local link](Papers/1612.01064v3.pdf), Hash: 3c9b0ed9eca03d76395f2c6afdf3e219, *Added: 2024-09-12* 

#### Proximal Policy Optimization Algorithms

*John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov*

**Summary:** We propose a new family of policy gradient methods for reinforcement learning, which al- ternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent.

**ArXiv:** [1707.06347](https://arxiv.org/abs/1707.06347), [Local link](Papers/1707.06347v2_PPO.pdf), Hash: 9e0725fc34e41616c0faab684513e464, *Added: 2024-09-12* 

#### SqueezeNext: Hardware-Aware Neural Network Design

*Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, Kurt Keutzer*

**Summary:** One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural networks.

**ArXiv:** [1803.10615](https://arxiv.org/abs/1803.10615), [Local link](Papers/1803.10615v2_squeezenext.pdf), Hash: ad6c65774f83c471c4529189e69999c6, *Added: 2024-09-12* 

#### Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions

*Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, Yingyan Lin*

**Summary:** We proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weight-sharing, by only recording Kcluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed k-means regularization, which tends to make hard assignments of convolutional layer weights to Klearned cluster centers during re-training.

**ArXiv:** [1806.09228](https://arxiv.org/abs/1806.09228), [Local link](Papers/1806.09228v1_palettization.pdf), Hash: 3988217769b90c5ec8a068684e558d8c, *Added: 2024-09-12* 


#### Learned Step Size Quantization

*Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S. Modha*

**Summary:** Learned Step Size Quantization presents a method for training deep networks with low precision operations at inference time, achieving high accuracy on the ImageNet dataset when using models with weights and activations quantized to 2-, 3- or 4-bits of precision.

**ArXiv:** [1902.08153](https://arxiv.org/abs/1902.08153), [Local link](Papers/1902.08153v3_good_paper_qat.pdf), Hash: 601fe34a0b921f764bd1c091b25b024b, *Added: 2024-09-12* 

#### All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification

*Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu*

**Summary:** Shift operation is an efficient alternative over depthwise separable convolution. However, it is still bottlenecked by its implementation manner, namely memory movement. To put this direction forward, a new and novel basic component named Sparse Shift Layer (SSL) is introduced in this paper to construct efficient convolutional neural networks.

**ArXiv:** [1903.05285](https://arxiv.org/abs/1903.05285), [Local link](Papers/1903.05285.pdf), Hash: b9e14495a718204775dd13877ae5c5a5, *Added: 2024-09-12* 

#### Character Region Awareness for Text Detection

*Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee*

**Summary:** This paper proposes a new scene text detection method that effectively detects text areas by exploring each character and affinity between characters.

**ArXiv:** [1904.01941](https://arxiv.org/abs/1904.01941), [Local link](Papers/1904.01941v1_craft_text_detection.pdf), Hash: c78b0a8bcb1f041fe659c25cd16832f9, *Added: 2024-09-12* 

#### Searching for MobileNetV3

*Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam*

**Summary:** We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design.

**ArXiv:** [1905.02244](https://arxiv.org/abs/1905.02244), [Local link](Papers/1905.02244v5_mobilenetv3.pdf), Hash: 6f29ad563a89d1086adf5e7f990c6e0a, *Added: 2024-09-12* 

#### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks

*Mingxing Tan, Quoc V. Le*

**Summary:** Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.

**ArXiv:** [1905.11946](https://arxiv.org/abs/1905.11946), [Local link](Papers/1905.11946_efficientnet.pdf), Hash: a6491e929ceb9af0db1f063a6a69fb61, *Added: 2024-09-12* 

#### DeepShift: Towards Multiplication-Less Neural Networks

*Mostafa Elhoushi, Zihao Chen, Farhan Shafig, Ye Henry Tian, Joey Yiwei Li*

**Summary:** The high computation, memory, and power budgets of inferring convolutional neural networks (CNNs) are major bottlenecks of model deployment to edge computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is time and energy-intensive even on high-grade servers.

**ArXiv:** [1905.13298](https://arxiv.org/abs/1905.13298), [Local link](Papers/1905.13298_deepshift.pdf), Hash: 94eeb0e12a131bbd5e31cec15e40b281, *Added: 2024-09-12* 

#### ADDITIVE POWERS -OF-TWO QUANTIZATION : AN EFFICIENT NON-UNIFORM DISCRETIZATION FOR NEURAL NETWORKS

*Yuhang Li, Xin Dong, Wei Wang*

**Summary:** We propose Additive Powers-of-Two (APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks.

**ArXiv:** [1909.13144](https://arxiv.org/abs/1909.13144), [Local link](Papers/1909.13144v2_expcoding.pdf), Hash: 614a358832156d33df4e2f86e8360e2c, *Added: 2024-09-12* 

#### AdderNet: Do We Really Need Multiplications in Deep Learning?

*Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu*

**Summary:** Adder Networks (AdderNets) trade massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs.

**ArXiv:** [1912.13200](https://arxiv.org/abs/1912.13200), [Local link](Papers/1912.13200.pdf), Hash: 8567dcb1010414feaac0223952396917, *Added: 2024-09-12* 

#### AdderSR: Towards Energy Efficient Image Super-Resolution

*Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, Dacheng Tao*

**Summary:** This paper studies the single image super-resolution problem using adder neural networks (AdderNets) and proposes AdderSR to achieve comparable performance with CNN baselines while reducing energy consumption by about 2.5 times.

**ArXiv:** [2009.08891](https://arxiv.org/abs/2009.08891), [Local link](Papers/2009.08891.pdf), Hash: ed2b2c74df22b3359386cf86ce266683, *Added: 2024-09-12* 

#### ANIMAGE IS WORTH 16X16 W ORDS: Transformers for Image Recognition at Scale

*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby*

**Summary:** We show that the reliance on CNNs in vision is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.

**ArXiv:** [2010.11929](https://arxiv.org/abs/2010.11929), [Local link](Papers/2010.11929.pdf), Hash: 7f1ba6e93e08ec7fc3f4fa1c8a255ac0, *Added: 2024-09-12* 

#### Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks

*TORSTEN HOEFLER, DAN ALISTARH, TAL BEN-NUN, NIKOLI DRYDEN, ALEXANDRA PESTE*

**Summary:** The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks.

**ArXiv:** [2102.00554](https://arxiv.org/abs/2102.00554), [Local link](Papers/2102.00554v1_sparsity.pdf), Hash: 2220739fe6d99860c784327d505b186d, *Added: 2024-09-12* 

#### VS-Q UANT: PER-VECTOR SCALED QUANTIZATION FOR ACCURATE LOW-PRECISION NEURAL NETWORK INFERENCE

*Steve Dai, Rangharajan Venkatesan, Haoxing Ren, Brian Zimmer, William J. Dally, Brucek Khailany*

**Summary:** Quantization enables efficient acceleration of deep neural networks by reducing model memory footprint and exploiting low-cost integer math hardware units.

**ArXiv:** [2102.04503](https://arxiv.org/abs/2102.04503), [Local link](Papers/2102.04503v1_per_vector_quant.pdf), Hash: b7fdb71333c901b39ecfc058d564e105, *Added: 2024-09-12* 

#### Network Quantization with Element-wise Gradient Scaling

*Junghyup Lee, Dohyung Kim, Bumsub Ham*

**Summary:** Network quantization aims at reducing bit-widths of weights and/or activations, particularly important for implementing deep neural networks with limited hardware resources. Most methods use the straight-through estimator (STE) to train quantized networks, which avoids a zero-gradient problem by replacing a derivative of a discretizer (i.e., a round function) with that of an identity function.

**ArXiv:** [2104.00903](https://arxiv.org/abs/2104.00903), [Local link](Papers/2104.00903v1_STE_Optimization_EWGS.pdf), Hash: b54c7ede8c2bafc99ef592b057b66e9d, *Added: 2024-09-12* 

#### LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference

*Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, HervÃ© JÃ©gou, Matthijs Douze*

**Summary:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware.

**ArXiv:** [2104.01136](https://arxiv.org/abs/2104.01136), [Local link](Papers/2104.01136v2_levit_light_weight_vision.pdf), Hash: c74f4b03484329d133a1188ab9c8bd4b, *Added: 2024-09-12* 

#### DKM: Differentiable k-Means Clustering Layer for Neural Network Compression

*Minsik Cho, Keivan Alizadeh-Vahid, Saurabh Adya, Mohammad Rastegari*

**Summary:** We propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight-clustering for DNN model compression.

**ArXiv:** [2108.12659v4](https://arxiv.org/abs/2108.12659v4), [Local link](Papers/2108.12659v4_palettization.pdf), Hash: 806791b8719a1735064e32741d0db5b7, *Added: 2024-09-12* 

#### Primer: Searching for Efficient Transformers for Language Modeling

*David R. So, Wojciech MaÅke, Hanxiao Liu, Zihang Dai, Noam Shazeer, Quoc V. Le*

**Summary:** Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant.

**ArXiv:** [2109.08668](https://arxiv.org/abs/2109.08668), [Local link](Papers/2109.08668v2_primer_llm_architecture_search.pdf), Hash: b0b2d5391fbd2c9ce4bdf4a29f18ef67, *Added: 2024-09-12* 

#### Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training

*Charbel Sakr, Steve Dai, Rangharajan Venkatesan, Brian Zimmer, William J. Dally, Brucek Khailany*

**Summary:** We propose Optimally Clipped Tensors And Vectors (OCTA V), a recursive algorithm to determine MSE-optimal clipping scalars for improved quantization-aware training, and reveal limitations in common gradient estimation techniques proposing magnitude-aware differentiation as a remedy.

**ArXiv:** [2206.06501](https://arxiv.org/abs/2206.06501), [Local link](Papers/2206.06501v1_optimal_clipping.pdf), Hash: 55168ecaa41e3f8fb55f87d69c766b7e, *Added: 2024-09-12* 

#### LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale

*Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer*

**Summary:** We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance.

**ArXiv:** [2208.07339](https://arxiv.org/abs/2208.07339), [Local link](Papers/2208.07339.pdf), Hash: cde5fd566b995573932664907c0eea57, *Added: 2024-09-12* 

#### A Curate Post-Training Quantization for Generative Pre-Trained Transformers

*Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh*

**Summary:** GPTQ, a new one-shot weight quantization method based on approximate second-order information, is proposed to reduce the bitwidth of GPT models with minimal accuracy degradation. This allows execution of an 175 billion-parameter model inside a single GPU for generative inference.

**ArXiv:** [2210.17323](https://arxiv.org/abs/2210.17323), [Local link](Papers/2210.17323.pdf), Hash: 1188fd9e7933a11cf51b835ac07aaa13, *Added: 2024-09-12* 

#### A Curate Post-Training Quantization for Generative Pre-Trained Transformers

*Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh*

**Summary:** GPTQ, a new one-shot weight quantization method based on approximate second-order information, efficiently reduces the bitwidth of GPT models to 3 or 4 bits per weight with negligible accuracy degradation.

**ArXiv:** [2210.17323](https://arxiv.org/abs/2210.17323), [Local link](Papers/2210.17323v2_GPTQ.pdf), Hash: 1188fd9e7933a11cf51b835ac07aaa13, *Added: 2024-09-12* 

#### Symbolic Discovery of Optimization Algorithms

*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*

**Summary:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training.

**ArXiv:** [2302.06675](https://arxiv.org/abs/2302.06675), [Local link](Papers/2302.06675v4_Lion_Opitmizer.pdf), Hash: 03ef2f8cf9b9a6b67b177bf857f5f057, *Added: 2024-09-12* 

#### Multiplication-Free Transformer Training via Piecewise Affine Operations

*Atli Kosson, Martin Jaggi*

**Summary:** We replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters.

**ArXiv:** [2305.17190](https://arxiv.org/abs/2305.17190), [Local link](Papers/2305.17190v2_mulfree.pdf), Hash: 269d214383aa4d76bb0d948da821b526, *Added: 2024-09-12* 

#### A WQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

*Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, Song Han*

**Summary:** This paper proposes Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for Large Language Model weight-only quantization, that searches for optimal per-channel scaling based on the observation of activation to protect salient weights.

**ArXiv:** [2306.00978](https://arxiv.org/abs/2306.00978), [Local link](Papers/2306.00978.pdf), Hash: e37ecfa8af69a03b4e126934caf4cee3, *Added: 2024-09-12* 

#### Patch nâ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution

*Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario LuÄiÄ, Neil Houlsby*

**Summary:** NaViT(Native Resolution ViT) uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios, demonstrating improved training efficiency for large-scale supervised and contrastive image-text pretraining.

**ArXiv:** [2302.08313](https://arxiv.org/abs/2302.08313), [Local link](Papers/2307.06304.pdf), Hash: afbbc25f4c7825c575cc757c566cd4f5, *Added: 2024-09-12* 

#### Explaining Grokking Through Circuit Efficiency

*Vikrant Varma, Rohin Shah, Zachary Kenton, JÃ¡nos KramÃ¡r, Ramana Kumar*

**Summary:** We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.

**ArXiv:** [2309.02390](https://arxiv.org/abs/2309.02390), [Local link](Papers/2309.02390v1_grokking.pdf), Hash: 11aa974cb9279a9217167206b42451f4, *Added: 2024-09-12* 

#### BitNet: Scaling 1-bit Transformers for Large Language Models

*Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei*

**Summary:** We introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. It reduces memory footprint and energy consumption while maintaining competitive performance.

**ArXiv:** [2310.11453](https://arxiv.org/abs/2310.11453), [Local link](Papers/2310.11453_bitnet.pdf), Hash: 2ad51decc58a80910bbd05b8084c676a, *Added: 2024-09-12* 

#### FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design

*Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song*

**Summary:** Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications.

**ArXiv:** [2401.14112](https://arxiv.org/abs/2401.14112), [Local link](Papers/2401.14112v2_fp6_llm.pdf), Hash: 7dbfccd4df499b1e6b6fd59857fe094e, *Added: 2024-09-12* 

#### MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

*Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra*

**Summary:** This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment.

**ArXiv:** [2402.14905](https://arxiv.org/abs/2402.14905), [Local link](Papers/2402.14905v2_mobilellm.pdf), Hash: 0fe2ef51d464a5cae1876f2382379428, *Added: 2024-09-12* 

#### The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

*Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei*

**Summary:** Recent research, such as BitNet [ WMD+23], is paving the way for a new era of 1- bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58 , in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.

**ArXiv:** [2402.17764](https://arxiv.org/abs/2402.17764), [Local link](Papers/2402.17764_1_58bit.pdf), Hash: 45f0b9c45de94bd864bd260cc10f40eb, *Added: 2024-09-12* 

#### The Unreasonable Ineffectiveness of the Deeper Layers

*Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts*

**Summary:** We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed.

**ArXiv:** [2403.17887](https://arxiv.org/abs/2403.17887), [Local link](Papers/2403.17887v1_layer_pruning.pdf), Hash: 652bf63701a5726bcffd722e555344b4, *Added: 2024-09-12* 

#### Tutorial on Diffusion Models for Imaging and Vision

*Stanley Chan*

**Summary:** The goal of this tutorial is to discuss the essential ideas underlying these diffusion models.

**ArXiv:** [2403.18103](https://arxiv.org/abs/2403.18103), [Local link](Papers/2403.18103v2_Tutorial_on_Diffusion_Models.pdf), Hash: bf04ad5ff022048df73bf8c3608ee803, *Added: 2024-09-12* 

#### Mixture-of-Depths: Dynamically allocating Compute in Transformer-Based Language Models

*David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam Santoro*

**Summary:** In this work, we demonstrate that transformers can learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimizing the allocation along the sequence for different layers across the model depth.

**ArXiv:** [2404.02258](https://arxiv.org/abs/2404.02258), [Local link](Papers/2404.02258.pdf), Hash: f90acf2f1085d7d99cd7c9f6aa908008, *Added: 2024-09-12* 

#### Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction

*Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang*

**Summary:** We present Visual AutoRegressive modeling (V AR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine "next-scale prediction" or "next-resolution prediction", diverging from the standard raster-scan "next-token prediction". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: V AR, for the first time , makes GPT-style AR models surpass diffusion transformers in image generation.

**ArXiv:** [2404.02905](https://arxiv.org/abs/2404.02905), [Local link](Papers/2404.02905.pdf), Hash: 4016f04c122748c124df09fe18b68492, *Added: 2024-09-12* 

#### Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference

*Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen GÃ¶rtler, Dominik Moritz, Jeffrey P. Bigham, Zhile Ren, Cecile Foret, Qi Shan, Xiaoyi Zhang*

**Summary:** On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences.

**ArXiv:** [2404.03085](https://arxiv.org/abs/2404.03085), [Local link](Papers/2404.03085v1_talaria.pdf), Hash: 8f4f4f0eda22a495f23c1337142e775b, *Added: 2024-09-12* 

#### Training LLMs over Neurally Compressed Text

*Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant*

**Summary:** In this paper, we explore the idea of training large language models (LLMs) over highly compressed text using Equal-Info Windows, a novel compression technique that segments text into blocks compressing to the same bit length.

**ArXiv:** [2404.03626](https://arxiv.org/abs/2404.03626), [Local link](Papers/2404.03626.pdf), Hash: 53529e681ab369bfee9e63d32619212c, *Added: 2024-09-12* 

#### Physics of Language Models: Part 3.3 Knowledge Capacity Scaling Laws

*Zeyuan Allen-Zhu, Yuanzhi Li*

**Summary:** Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores.

**ArXiv:** [2404.05405](https://arxiv.org/abs/2404.05405), [Local link](Papers/2404.05405v1_physics_of_llm.pdf), Hash: 682a50a5b5813643d0afc1a494abe7fc, *Added: 2024-09-12* 

#### Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization

*Boshi Wang, Xiang Yue, Yu Su, Huan Sun*

**Summary:** We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers canlearn implicit reasoning, but only through grokking , i.e., extended training far beyond overfitting.

**ArXiv:** [2405.15071](https://arxiv.org/abs/2405.15071), [Local link](Papers/2405.15071v1_grokking.pdf), Hash: 259f8dbea45f5563ea5f2d9b166d44ec, *Added: 2024-09-12* 

#### Scalable MatMul-free Language Modeling

*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian*

**Summary:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales.

**ArXiv:** [2406.02528](https://arxiv.org/abs/2406.02528), [Local link](Papers/2406.02528v1_matmul_free.pdf), Hash: dc43e3bf03f0f3882f1e15b331027f19, *Added: 2024-09-12* 

#### Q-Sparse: All Large Language Models can be Fully Sparsely-Activated

*Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei*

**Summary:** We introduce, Q-Sparse , a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of acti-vations in LLMs which can bring significant efficiency gains in inference.

**ArXiv:** [2407.10969](https://arxiv.org/abs/2407.10969), [Local link](Papers/2407.10969v1_Q-sparse.pdf), Hash: fd79b84fc95a6af58f65017ab49255fa, *Added: 2024-09-12* 

#### ShiftAddNet: A Hardware-Inspired Deep Network

*Haoran You, Xiaohan Chen, Yongan Zhang, Chaojian Li, Sicheng Li, Zihao Liu, Zhangyang Wang, Yingyan Lin*

**Summary:** This paper introduces ShiftAddNet, a deep network inspired by energy-efficient hardware implementation that performs multiplication using additions and logical bit-shifts, resulting in energy-efficient inference and training without compromising expressive capacity.

**ArXiv:** [1905.13298](https://arxiv.org/abs/1905.13298), [Local link](Papers/NeurIPS-2020-shiftaddnet-a-hardware-inspired-deep-network-Paper.pdf), Hash: bae3561ed2129c272887af8ce50df626, *Added: 2024-09-12* 

#### Optimal Brain Damage

*Yann Le Cun, John S. Denker, Sara A. Solla*

**Summary:** We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network.

**ArXiv:** N/A, [Local link](Papers/NIPS-1989-optimal-brain-damage-Paper.pdf), Hash: c1bfc00c11a88f7d9abbc1615f100613, *Added: 2024-09-12* 

#### OCP Microscaling Formats (MX) Specification

*Bita Darvish Rouhani, Nitin Garegrat, Tom Savell, Ankit More, Kyung-Nam Han, Ritchie Zhao, Mathew Hall, Jasmine Klar, Eric Chung, Yuan Yu, Microsoft, Michael Schulte, Ralph Wittig, Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Pradeep Dubey, Marius Cornea, Alexander Heinecke, Andres Rodriguez, Martin Langhammer, Summer Deng, Maxim Naumov, Paulius Micikevicius, Michael Siu, Colin Verrilli*

**Summary:** This document describes the OCP Microscaling Formats (MX) Specification, which includes definitions, word usage, overview, basic operations, and more.

**ArXiv:** N/A, [Local link](Papers/OCP_Microscaling%20Formats%20(MX)%20v1.0%20Spec_Final.pdf), Hash: 9a52e4a8d4a28b49ecef2a3a3f918337, *Added: 2024-09-12* 

#### Improving Neural Network Quantization without Retraining using Outlier Channel Splitting

*Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang*

**Summary:** Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training.

**ArXiv:** [1901.09504](https://arxiv.org/abs/1901.09504), [Local link](Papers/1901.09504v3_outlier%20channel%20splitting.pdf), Hash: 745960627406b452459035a5285e7824, *Added: 2024-09-12* 

#### DKM: Differentiable k-Means Clustering Layer for Neural Network Compression

*Minsik Cho, Keivan Alizadeh-Vahid, Saurabh Adya, Mohammad Rastegari*

**Summary:** This paper proposes a differentiable k-means clustering layer (DKM) and its application to train-time weight-clustering for deep neural network model compression. DKM enables joint optimization of the DNN parameters and clustering centroids, providing superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks.

**ArXiv:** [2108.12659v4](https://arxiv.org/abs/2108.12659v4), [Local link](Papers/2108.12659v4_palettization.pdf), Hash: 806791b8719a1735064e32741d0db5b7, *Added: 2024-09-12* 

#### Rare Gems: Finding Lottery Tickets at Initialization

*Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grindenwalt Nagle, Hongyi Wang, Eric Xing, Kangwook Lee, Dimitris Papailiopoulos*

**Summary:** GEM-MINER finds lottery tickets at initialization that beat current baselines, finding trainable networks to accuracy competitive or better than Iterative Magnitude Pruning (IMP), and does so up to 19 times faster.

**ArXiv:** [2202.12002](https://arxiv.org/abs/2202.12002), [Local link](Papers/2202.12002v2_Finding%20Lottery%20Tickets%20at%20Initialization.pdf), Hash: 4823419ab710161dca0fd8515e29582f, *Added: 2024-09-12* 

#### No Free Prune: Information-Theoretic Barriers to Pruning at Initialization

*Tanishq Kumar, Kevin Luo, Mark Sellke*

**Summary:** The existence of lottery tickets Frankle & Carbin (2018) at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ('pruning at initialization') have been broadly unsuccessful Frankle et al. (2020b).

**ArXiv:** [2402.01089](https://arxiv.org/abs/2402.01089), [Local link](Papers/2402.01089v1_No%20Free%20Prune.pdf), Hash: e6582e9efd12f859cbd8be9953195142, *Added: 2024-09-12* 

#### Least Squares Quantization in PCM

*Stuart P. Lloyd*

**Summary:** In this paper, the author derives necessary conditions for optimum finite quantization schemes with a given ensemble of signals to handle, using the optimization criterion that the average quantization noise power be a minimum.

**ArXiv:** N/A, [Local link](Papers/lloyd1982_least_squares_quant.pdf), Hash: e8cfe41ea5354727e98fe6ffd6d7e2e1, *Added: 2024-09-12* 

#### Optimal Brain Damage

*Yann Le Cun, John S. Denker, Sara A. Solla*

**Summary:** We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network.

**ArXiv:** N/A, [Local link](Papers/NIPS-1989-optimal-brain-damage-Paper.pdf), Hash: c1bfc00c11a88f7d9abbc1615f100613, *Added: 2024-09-12* 

#### OCP Microscaling Formats (MX) Specification

*Bita Darvish Rouhani, Nitin Garegrat, Tom Savell, Ankit More, Kyung-Nam Han, Ritchie Zhao, Mathew Hall, Jasmine Klar, Eric Chung, Yuan Yu, Microsoft, Michael Schulte, Ralph Wittig, Ian Bratt, Nigel Stephens, Jelena Milanovic, John Brothers, Pradeep Dubey, Marius Cornea, Alexander Heinecke, Andres Rodriguez, Martin Langhammer, Summer Deng, Maxim Naumov, Paulius Micikevicius, Michael Siu, Colin Verrilli*

**Summary:** This Specification defines the Microscaling (MX) format for use in Open Compute Project (OCP) designs to improve efficiency and performance.

**ArXiv:** N/A, [Local link](Papers/OCP_Microscaling%20Formats%20%28MX%29%20v1.0%20Spec_Final.pdf), Hash: 9a52e4a8d4a28b49ecef2a3a3f918337, *Added: 2024-09-12* 

